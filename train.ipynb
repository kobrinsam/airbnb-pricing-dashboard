{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Script\n",
    "This script extracts data from the snowflake table, transforms the features, and then traings the model on the engeneered features. Our training script leverages MLflow for model logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install snowflake-connector-python\n",
    "#!pip install mlflow mlflow==1.22.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "\n",
    "# Enable pandas to display up to 500 columns\n",
    "pd.set_option('display.max_columns', 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load Snowflake credentials for temp user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    TEMP_USER = os.getenv('SNOWSQL_TEMP_USER')\n",
    "    TEMP_USER_PASSWORD = os.getenv('SNOWSQL_TEMP_PWD')\n",
    "    \n",
    "    if not TEMP_USER:\n",
    "        raise ValueError(\"Environment variable SNOWSQL_TEMP_USER must be set\")\n",
    "    if not TEMP_USER_PASSWORD:\n",
    "        raise ValueError(\"Environment variable SNOWSQL_TEMP_PWD must be set\")\n",
    "\n",
    "except ValueError as ve:\n",
    "    print(f\"Error: {ve}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Snowflake successfully at 2024-07-28 20:0317\n"
     ]
    }
   ],
   "source": [
    "# Establish connection to Snowflake\n",
    "current_time = datetime.now().strftime('%Y-%m-%d %H:%M%S')\n",
    "\n",
    "try:\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=TEMP_USER,\n",
    "        password=TEMP_USER_PASSWORD,\n",
    "        account='ygeuort-alb19263',\n",
    "        warehouse='COMPUTE_WH',\n",
    "        database='AIRBNB',\n",
    "        schema='ODS'\n",
    "    )\n",
    "\n",
    "    print(f'Connected to Snowflake successfully at {current_time}')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Failed to connect to Snowflake on {current_time} due to error code {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sql_query, date_columns=None):\n",
    "    \"\"\"\n",
    "    Executes a SQL query and returns the result as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        sql_query (str): SQL query to execute.\n",
    "\n",
    "    Returns:\n",
    "        df_result: Resulting DataFrame from the SQL query.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(sql_query)\n",
    "\n",
    "        # load data into dataframe\n",
    "        df_result = cursor.fetch_pandas_all()\n",
    "\n",
    "        # Convert column names to lowercase\n",
    "        df_result.columns = map(str.lower, df_result.columns)\n",
    "\n",
    "        # Parse specified date columns\n",
    "        if date_columns:\n",
    "            for col in date_columns:\n",
    "                df_result[col] = pd.to_datetime(df_result[col], errors='coerce')\n",
    "\n",
    "    finally:\n",
    "        if cursor is not None:\n",
    "            cursor.close()\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract data for all markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111052, 68)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = '''\n",
    "select * from listings\n",
    " '''\n",
    "\n",
    "df_raw = get_data(sql_query)\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target for the model\n",
    "features = ['market', 'room_type', 'accommodates', 'bathrooms', 'beds', 'latitude', 'longitude', 'amenities']\n",
    "categorical_features = ['market', 'room_type']  # Features with categorical data\n",
    "numerical_features = ['accommodates', 'bathrooms', 'beds', 'latitude', 'longitude']  # Features with numerical data\n",
    "text_features = ['amenities']  # (Optional) Feature for text data like amenities\n",
    "target = 'price'  # Target variable to predict\n",
    "\n",
    "# Filter the dataframe to include only the selected features and target column\n",
    "df = df_raw[features + [target]]\n",
    "\n",
    "# Remove rows where the price exceeds $600 per night to avoid outliers\n",
    "df = df[df[target] <= 600]\n",
    "\n",
    "# Drop rows with missing values in the target column to ensure data integrity\n",
    "df = df.dropna(subset=[target])\n",
    "\n",
    "# Separate the features (X) and the target (y)\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Split the data into training and test sets to evaluate model performance. Note because we have a large dataset, 111,052 obsevations, we can use a smaller test size of 10% the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation MAE scores: [46.68859149 45.83769521 46.89561522 46.04752452 45.3387151  46.27384157\n",
      " 47.72848338 46.45816514 44.7925808  46.91317849]\n",
      "Average cross-validation MAE: 46.29743909095708\n",
      "Cross-validation RMSE scores: [71.39870686 69.99117595 71.61107009 69.75335034 67.972491   69.71784132\n",
      " 71.65582086 69.93141402 69.28409406 70.97600159]\n",
      "Average cross-validation RMSE: 70.22919660817408\n",
      "Test set MAE: 46.041895386942365\n",
      "Test set RMSE: 69.08103970267469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['regression_pipeline.joblib']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get model run start time\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Define the pipeline\n",
    "def createPipeline(numerical_features, categorical_features):\n",
    "    # Define the preprocessing for numerical features\n",
    "    numericalTransformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Define the preprocessing for categorical features\n",
    "    categoricalTransformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Combine preprocessing for numerical and categorical features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numericalTransformer, numerical_features),\n",
    "            ('cat', categoricalTransformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', RandomForestRegressor(max_depth=30,\n",
    "                                            max_features='auto',\n",
    "                                            min_samples_leaf=2,\n",
    "                                            min_samples_split=2,\n",
    "                                            n_estimators=300,\n",
    "                                            random_state=42)) #best model after hyperparameter tuning\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def log_to_file(log_file, params, metrics, training_details):\n",
    "    # Load existing data\n",
    "    try:\n",
    "        with open(log_file, 'r') as f:\n",
    "            logs = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        logs = []\n",
    "\n",
    "    # Append new log entry\n",
    "    log_entry = {\n",
    "        \"params\": params,\n",
    "        \"metrics\": metrics,\n",
    "        \"training_details\": training_details\n",
    "    }\n",
    "    logs.append(log_entry)\n",
    "\n",
    "    # Write updated logs back to the file\n",
    "    with open(log_file, 'w') as f:\n",
    "        json.dump(logs, f, indent=4)\n",
    "\n",
    "# Parameters and file paths\n",
    "log_file = 'experiment_log.json'\n",
    "model_file = 'regression_pipeline.joblib'\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = createPipeline(numerical_features, categorical_features)\n",
    "\n",
    "# Perform cross-validation with Mean Absolute Error (MAE)\n",
    "maeScores = cross_val_score(pipeline, X_train, y_train, cv=10, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "maeScores = -maeScores  # Convert negative MAE to positive\n",
    "print(\"Cross-validation MAE scores:\", maeScores)\n",
    "print(\"Average cross-validation MAE:\", maeScores.mean())\n",
    "\n",
    "# Perform cross-validation with Root Mean Squared Error (RMSE)\n",
    "rmse_scorer = make_scorer(mean_squared_error, squared=False)  # squared=False returns RMSE\n",
    "rmseScores = cross_val_score(pipeline, X_train, y_train, cv=10, scoring=rmse_scorer, n_jobs=-1)\n",
    "print(\"Cross-validation RMSE scores:\", rmseScores)\n",
    "print(\"Average cross-validation RMSE:\", rmseScores.mean())\n",
    "\n",
    "# Parameters and metrics to log\n",
    "params = {\n",
    "    \"model_type\": str(pipeline.get_params()['regressor']),\n",
    "    \"numerical_features\": numerical_features,\n",
    "    \"categorical_features\": categorical_features\n",
    "}\n",
    "metrics = {\n",
    "    \"cv_mae_mean\": maeScores.mean(),\n",
    "    \"cv_rmse_mean\": rmseScores.mean(),\n",
    "    \"cv_mae_std\": maeScores.std(),\n",
    "    \"cv_rmse_std\": rmseScores.std()\n",
    "}\n",
    "training_details = {\n",
    "    \"train_size\": len(X_train),\n",
    "    \"test_size\": len(X_test),\n",
    "    \"train_duration\": str(datetime.now() - start_time)\n",
    "}\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model on the test set using MAE\n",
    "y_pred = pipeline.predict(X_test)\n",
    "mae_test = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Test set MAE:\", mae_test)\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "rmse_test = mean_squared_error(y_test, y_pred, squared=False)  # squared=False returns RMSE\n",
    "print(\"Test set RMSE:\", rmse_test)\n",
    "\n",
    "# Log training run results to file\n",
    "metrics.update({\"test_mae\": mae_test, \"test_rmse\": rmse_test})\n",
    "log_to_file(log_file, params, metrics, training_details)\n",
    "\n",
    "# Refit the final model on the entire dataset to improve generalization by leveraging all available information\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(pipeline, model_file, compress=('gzip', 9))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
