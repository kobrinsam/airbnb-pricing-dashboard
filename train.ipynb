{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Script\n",
    "This script extracts data from the snowflake table, transforms the features, and then traings the model on the engeneered features. Our training script leverages MLflow for model logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "#import functions from helper_functions.py\n",
    "from helper_functions import get_data, write_to_snowflake\n",
    "# Enable pandas to display up to 500 columns\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(current_directory, '..')))\n",
    "\n",
    "# Import the helper_functions module\n",
    "from helper_functions import connect_to_snowflake, get_data, write_to_snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Snowflake using schema ODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Snowflake schema ODS\n"
     ]
    }
   ],
   "source": [
    "conn = connect_to_snowflake(schema_name='ODS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract data for all markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111052, 68)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = '''\n",
    "select * from ods.listings\n",
    " '''\n",
    "\n",
    "df_raw = get_data(sql_query, conn=conn)\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target for the model\n",
    "features = ['market', 'room_type', 'accommodates', 'bathrooms', 'beds', 'latitude', 'longitude', 'amenities']\n",
    "categorical_features = ['market', 'room_type']  # Features with categorical data\n",
    "numerical_features = ['accommodates', 'bathrooms', 'beds', 'latitude', 'longitude']  # Features with numerical data\n",
    "text_features = ['amenities']  # (Optional) Feature for text data like amenities\n",
    "target = 'price'  # Target variable to predict\n",
    "\n",
    "# Filter the dataframe to include only the selected features and target column\n",
    "df = df_raw[features + [target]]\n",
    "\n",
    "# Remove NA values from the 'price' column\n",
    "df_raw_no_na = df_raw[target].dropna()\n",
    "# Remove rows where the price exceeds $600 per night (97th percentile) to avoid outliers, consider dropping vals <40 (3rd percentile)\n",
    "df = df[df[target] <= 600]\n",
    "\n",
    "# Drop rows with missing values in the target column to ensure data integrity\n",
    "df = df.dropna(subset=[target])\n",
    "# write cleaned data to snowflake feature store\n",
    "write_to_snowflake(df,conn=conn, snowflake_schema_name='FEATURE_STORE', snowflake_table_name='listings_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add code for pulling from snowflake feature store\n",
    "\n",
    "# Separate the features (X) and the target (y)\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Split the data into training and test sets to evaluate model performance. Note because we have a large dataset, 111,052 obsevations, we can use a smaller test size of 10% the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation MAE scores: [46.92844899 45.71262102 46.13681104 46.18273602 46.38888795 46.11667545\n",
      " 46.19433292 46.97021873 46.64408421 46.03438575]\n",
      "Average cross-validation MAE: 46.330920206389905\n",
      "Cross-validation RMSE scores: [70.48089713 69.07791366 69.85507162 70.81930847 70.48595348 69.76894344\n",
      " 69.36436791 71.07897349 71.5754946  69.38603469]\n",
      "Average cross-validation RMSE: 70.1892958498672\n",
      "Test set MAE: 46.16088187131708\n",
      "Test set RMSE: 69.43573853209195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Venv\\analytics\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model/regression_pipeline.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get model run start time\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Define the pipeline\n",
    "def createPipeline(numerical_features, categorical_features):\n",
    "    # Define the preprocessing for numerical features\n",
    "    numericalTransformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Define the preprocessing for categorical features\n",
    "    categoricalTransformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Combine preprocessing for numerical and categorical features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numericalTransformer, numerical_features),\n",
    "            ('cat', categoricalTransformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', RandomForestRegressor(max_depth=30,\n",
    "                                            max_features=None,\n",
    "                                            min_samples_leaf=2,\n",
    "                                            min_samples_split=2,\n",
    "                                            n_estimators=300,\n",
    "                                            random_state=42)) #best model after hyperparameter tuning\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def log_to_file(log_file, params, metrics, training_details):\n",
    "    # Load existing data\n",
    "    try:\n",
    "        with open(log_file, 'r') as f:\n",
    "            logs = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        logs = []\n",
    "\n",
    "    # Append new log entry\n",
    "    log_entry = {\n",
    "        \"params\": params,\n",
    "        \"metrics\": metrics,\n",
    "        \"training_details\": training_details\n",
    "    }\n",
    "    logs.append(log_entry)\n",
    "\n",
    "    # Write updated logs back to the file\n",
    "    with open(log_file, 'w') as f:\n",
    "        json.dump(logs, f, indent=4)\n",
    "\n",
    "# Parameters and file paths\n",
    "log_file = 'model/experiment_log.json'\n",
    "model_file = 'model/regression_pipeline.joblib'\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = createPipeline(numerical_features, categorical_features)\n",
    "\n",
    "# Perform cross-validation with Mean Absolute Error (MAE)\n",
    "maeScores = cross_val_score(pipeline, X_train, y_train, cv=10, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "maeScores = -maeScores  # Convert negative MAE to positive\n",
    "print(\"Cross-validation MAE scores:\", maeScores)\n",
    "print(\"Average cross-validation MAE:\", maeScores.mean())\n",
    "\n",
    "# Perform cross-validation with Root Mean Squared Error (RMSE)\n",
    "rmse_scorer = make_scorer(mean_squared_error, squared=False)  # squared=False returns RMSE\n",
    "rmseScores = cross_val_score(pipeline, X_train, y_train, cv=10, scoring=rmse_scorer, n_jobs=-1)\n",
    "print(\"Cross-validation RMSE scores:\", rmseScores)\n",
    "print(\"Average cross-validation RMSE:\", rmseScores.mean())\n",
    "\n",
    "# Parameters and metrics to log\n",
    "params = {\n",
    "    \"model_type\": str(pipeline.get_params()['regressor']),\n",
    "    \"numerical_features\": numerical_features,\n",
    "    \"categorical_features\": categorical_features\n",
    "}\n",
    "metrics = {\n",
    "    \"cv_mae_mean\": maeScores.mean(),\n",
    "    \"cv_rmse_mean\": rmseScores.mean(),\n",
    "    \"cv_mae_std\": maeScores.std(),\n",
    "    \"cv_rmse_std\": rmseScores.std()\n",
    "}\n",
    "training_details = {\n",
    "    \"train_size\": len(X_train),\n",
    "    \"test_size\": len(X_test),\n",
    "    \"train_duration\": str(datetime.now() - start_time)\n",
    "}\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model on the test set using MAE\n",
    "y_pred = pipeline.predict(X_test)\n",
    "mae_test = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Test set MAE:\", mae_test)\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "rmse_test = mean_squared_error(y_test, y_pred, squared=False)  # squared=False returns RMSE\n",
    "print(\"Test set RMSE:\", rmse_test)\n",
    "\n",
    "# Log training run results to file\n",
    "metrics.update({\"test_mae\": mae_test, \"test_rmse\": rmse_test})\n",
    "#log_to_file(log_file, params, metrics, training_details)\n",
    "\n",
    "# Refit the final model on the entire dataset to improve generalization by leveraging all available information\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(pipeline, model_file, compress=('gzip', 9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
