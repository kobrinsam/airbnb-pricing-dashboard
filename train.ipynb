{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Script\n",
    "This script extracts data from the snowflake table, transforms the features, and then traings the model on the engeneered features. Our training script leverages MLflow for model logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "#import functions from helper_functions.py\n",
    "from helper_functions import get_data, write_to_snowflake\n",
    "# Enable pandas to display up to 500 columns\n",
    "pd.set_option('display.max_columns', 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load Snowflake credentials for temp user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    TEMP_USER = os.getenv('SNOWSQL_TEMP_USER')\n",
    "    TEMP_USER_PASSWORD = os.getenv('SNOWSQL_TEMP_PWD')\n",
    "    \n",
    "    if not TEMP_USER:\n",
    "        raise ValueError(\"Environment variable SNOWSQL_TEMP_USER must be set\")\n",
    "    if not TEMP_USER_PASSWORD:\n",
    "        raise ValueError(\"Environment variable SNOWSQL_TEMP_PWD must be set\")\n",
    "\n",
    "except ValueError as ve:\n",
    "    print(f\"Error: {ve}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Snowflake successfully at 2024-07-30 20:0439\n"
     ]
    }
   ],
   "source": [
    "# Establish connection to Snowflake\n",
    "current_time = datetime.now().strftime('%Y-%m-%d %H:%M%S')\n",
    "\n",
    "try:\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=TEMP_USER,\n",
    "        password=TEMP_USER_PASSWORD,\n",
    "        account='ygeuort-alb19263',\n",
    "        warehouse='COMPUTE_WH',\n",
    "        database='AIRBNB',\n",
    "    )\n",
    "\n",
    "    print(f'Connected to Snowflake successfully at {current_time}')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Failed to connect to Snowflake on {current_time} due to error code {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract data for all markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111052, 68)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = '''\n",
    "select * from ods.listings\n",
    " '''\n",
    "\n",
    "df_raw = get_data(sql_query, conn=conn)\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samkobrin/Documents/GitHub/airbnb-pricing-dashboard/helper_functions.py:56: UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.numeric.Int64Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)\n",
      "  success, num_chunks, num_rows, output = write_pandas(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table and data loaded to Snowflake at 2024-07-30 20:1936\n"
     ]
    }
   ],
   "source": [
    "# Select features and target for the model\n",
    "features = ['market', 'room_type', 'accommodates', 'bathrooms', 'beds', 'latitude', 'longitude', 'amenities']\n",
    "categorical_features = ['market', 'room_type']  # Features with categorical data\n",
    "numerical_features = ['accommodates', 'bathrooms', 'beds', 'latitude', 'longitude']  # Features with numerical data\n",
    "target = 'price'  # Target variable to predict\n",
    "\n",
    "# Filter the dataframe to include only the selected features and target column\n",
    "df = df_raw[features + [target]]\n",
    "\n",
    "# Remove NA values from the 'price' column\n",
    "df_raw_no_na = df_raw[target].dropna()\n",
    "# Remove rows where the price exceeds $600 per night (97th percentile) to avoid outliers\n",
    "df = df[df[target] <= 600]\n",
    "\n",
    "# Drop rows with missing values in the target column to ensure data integrity\n",
    "df = df.dropna(subset=[target])\n",
    "# write cleaned data to snowflake feature store\n",
    "write_to_snowflake(df,conn=conn, snowflake_schema_name='FEATURE_STORE', snowflake_table_name='listings_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add code for pulling from snowflake feature store\n",
    "\n",
    "# Separate the features (X) and the target (y)\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Split the data into training and test sets to evaluate model performance. Note because we have a large dataset, 111,052 obsevations, we can use a smaller test size of 10% the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation MAE scores: [45.66534262 46.30377953 46.52061509 46.76627372 47.00417896 46.03335114\n",
      " 45.68168021 45.95040059 46.96100644 45.72665094]\n",
      "Average cross-validation MAE: 46.26132792400367\n",
      "Cross-validation RMSE scores: [69.5905525  70.35051165 70.57718629 70.92256786 72.49644571 68.91319669\n",
      " 69.19007214 69.40628224 70.79759222 68.86378946]\n",
      "Average cross-validation RMSE: 70.11081967490922\n",
      "Test set MAE: 46.140358929648826\n",
      "Test set RMSE: 69.5552348912378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model/regression_pipeline.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get model run start time\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Define the pipeline\n",
    "def createPipeline(numerical_features, categorical_features):\n",
    "    # Define the preprocessing for numerical features\n",
    "    numericalTransformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Define the preprocessing for categorical features\n",
    "    categoricalTransformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Combine preprocessing for numerical and categorical features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numericalTransformer, numerical_features),\n",
    "            ('cat', categoricalTransformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', RandomForestRegressor(max_depth=30,\n",
    "                                            max_features='auto',\n",
    "                                            min_samples_leaf=2,\n",
    "                                            min_samples_split=2,\n",
    "                                            n_estimators=300,\n",
    "                                            random_state=42)) #best model after hyperparameter tuning\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def log_to_file(log_file, params, metrics, training_details):\n",
    "    # Load existing data\n",
    "    try:\n",
    "        with open(log_file, 'r') as f:\n",
    "            logs = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        logs = []\n",
    "\n",
    "    # Append new log entry\n",
    "    log_entry = {\n",
    "        \"params\": params,\n",
    "        \"metrics\": metrics,\n",
    "        \"training_details\": training_details\n",
    "    }\n",
    "    logs.append(log_entry)\n",
    "\n",
    "    # Write updated logs back to the file\n",
    "    with open(log_file, 'w') as f:\n",
    "        json.dump(logs, f, indent=4)\n",
    "\n",
    "# Parameters and file paths\n",
    "log_file = 'model/experiment_log.json'\n",
    "model_file = 'model/regression_pipeline.joblib'\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = createPipeline(numerical_features, categorical_features)\n",
    "\n",
    "# Perform cross-validation with Mean Absolute Error (MAE)\n",
    "maeScores = cross_val_score(pipeline, X_train, y_train, cv=10, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "maeScores = -maeScores  # Convert negative MAE to positive\n",
    "print(\"Cross-validation MAE scores:\", maeScores)\n",
    "print(\"Average cross-validation MAE:\", maeScores.mean())\n",
    "\n",
    "# Perform cross-validation with Root Mean Squared Error (RMSE)\n",
    "rmse_scorer = make_scorer(mean_squared_error, squared=False)  # squared=False returns RMSE\n",
    "rmseScores = cross_val_score(pipeline, X_train, y_train, cv=10, scoring=rmse_scorer, n_jobs=-1)\n",
    "print(\"Cross-validation RMSE scores:\", rmseScores)\n",
    "print(\"Average cross-validation RMSE:\", rmseScores.mean())\n",
    "\n",
    "# Parameters and metrics to log\n",
    "params = {\n",
    "    \"model_type\": str(pipeline.get_params()['regressor']),\n",
    "    \"numerical_features\": numerical_features,\n",
    "    \"categorical_features\": categorical_features\n",
    "}\n",
    "metrics = {\n",
    "    \"cv_mae_mean\": maeScores.mean(),\n",
    "    \"cv_rmse_mean\": rmseScores.mean(),\n",
    "    \"cv_mae_std\": maeScores.std(),\n",
    "    \"cv_rmse_std\": rmseScores.std()\n",
    "}\n",
    "training_details = {\n",
    "    \"train_size\": len(X_train),\n",
    "    \"test_size\": len(X_test),\n",
    "    \"train_duration\": str(datetime.now() - start_time)\n",
    "}\n",
    "\n",
    "# Train the model on the entire training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model on the test set using MAE\n",
    "y_pred = pipeline.predict(X_test)\n",
    "mae_test = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Test set MAE:\", mae_test)\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "rmse_test = mean_squared_error(y_test, y_pred, squared=False)  # squared=False returns RMSE\n",
    "print(\"Test set RMSE:\", rmse_test)\n",
    "\n",
    "# Log training run results to file\n",
    "metrics.update({\"test_mae\": mae_test, \"test_rmse\": rmse_test})\n",
    "#log_to_file(log_file, params, metrics, training_details)\n",
    "\n",
    "# Refit the final model on the entire dataset to improve generalization by leveraging all available information\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(pipeline, model_file, compress=('gzip', 9))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
