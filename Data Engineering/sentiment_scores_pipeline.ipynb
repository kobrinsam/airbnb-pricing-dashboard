{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Scores Pipeline\n",
    "#### Tokenize Reviews and Generate Sentiment Scores\n",
    "This notebook will load review data from the ODS schema by cleaning comments and calculating sentiment scores.  The pipeline contains the following steps:\n",
    "- Review data is fetched from the ODS (Operational Data Store) in Snowflake.\n",
    "- Reviews are cleaned and tokenized.\n",
    "- Sentiment scores are calculated for each review.\n",
    "- Data is uploaded into Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(current_directory, '..')))\n",
    "\n",
    "# Import the helper_functions module\n",
    "from helper_functions import connect_to_snowflake, get_data, write_to_snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\susac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\susac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\susac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\susac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\susac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import Data\n",
    "#### 1.1 Connect to Snowflake using schema ODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Snowflake schema ODS\n"
     ]
    }
   ],
   "source": [
    "conn = connect_to_snowflake(schema_name='ODS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup functions\n",
    "#### 2.1 Comment cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Sentiment analyzer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to compute sentiment score\n",
    "def get_sentiment_score(text):\n",
    "    return sid.polarity_scores(text)['compound']\n",
    "\n",
    "\n",
    "def apply_sentiment_score(df):\n",
    "    # Apply sentiment analysis to each cleaned comment\n",
    "    df['sentiment_score'] = df['clean_comments'].apply(get_sentiment_score)\n",
    "\n",
    "    # Classify sentiment based on score\n",
    "    df['sentiment'] = df['sentiment_score'].apply(lambda x: 'positive' if x >= 0.05 else ('negative' if x <= -0.05 else 'neutral'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Create pipeline to process reviews to Snowflake\n",
    "Processed data will be uploaded to the schema FEATURE_STORE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_upload_sentiment_scores(markets):\n",
    "    \"\"\"\n",
    "    Processes sentiment scores for each market and uploads results to Snowflake.\n",
    "\n",
    "    Parameters:\n",
    "    - markets (List of strings): List of markets to process.\n",
    "    \"\"\"\n",
    "    # Set this to true so that table is created during first run\n",
    "    create_table = True\n",
    "\n",
    "    for market in markets:\n",
    "    \n",
    "        print(f\"Running pipeline for market: {market}\")\n",
    "\n",
    "        # Fetch data for the current market\n",
    "        print(f'Fetching data')\n",
    "        sql_query = f\"SELECT * FROM REVIEWS WHERE market = '{market}'\"\n",
    "        df = get_data(sql_query, conn)\n",
    "\n",
    "        print('Applying clean_text function')\n",
    "        # Clean text data\n",
    "        df['clean_comments'] = df['comments'].apply(clean_text)\n",
    "\n",
    "        print('Generating sentiment scores')\n",
    "        # Apply sentiment scores\n",
    "        df = apply_sentiment_score(df)\n",
    "\n",
    "        # Capitalize column names prior to writing to Snowflake\n",
    "        df.columns = [col.upper() for col in df.columns]\n",
    "\n",
    "        print('Writing to Snowflake')\n",
    "        # Write to Snowflake\n",
    "        write_to_snowflake(df_name=df,conn=conn, schema_name='FEATURE_STORE',\n",
    "                           table_name='REVIEWS_SENTIMENT_SCORES', overwrite_table=create_table)\n",
    "        \n",
    "        print('----------------------------------------------------------------')\n",
    "\n",
    "        # Set overwrite_table to False after the first run so that data is appended\n",
    "        if create_table:\n",
    "            create_table = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply processing pipeline\n",
    "The first run through the pipeline auto creates a table and pushes the data to Snowflake and each subsequent run through the pipeline appends the market data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline for market: albany\n",
      "Fetching data\n",
      "Applying clean_text function\n",
      "Generating sentiment scores\n",
      "Writing to Snowflake\n",
      "Table REVIEWS_SENTIMENT_SCORES created in schema FEATURE_STORE at 2024-08-11 17:2026\n",
      "----------------------------------------------------------------\n",
      "Running pipeline for market: chicago\n",
      "Fetching data\n",
      "Applying clean_text function\n",
      "Generating sentiment scores\n",
      "Writing to Snowflake\n",
      "Data appended to table REVIEWS_SENTIMENT_SCORES in schema FEATURE_STORE at 2024-08-11 17:2227\n",
      "----------------------------------------------------------------\n",
      "Running pipeline for market: los-angeles\n",
      "Fetching data\n",
      "Applying clean_text function\n",
      "Generating sentiment scores\n",
      "Writing to Snowflake\n",
      "Data appended to table REVIEWS_SENTIMENT_SCORES in schema FEATURE_STORE at 2024-08-11 17:3148\n",
      "----------------------------------------------------------------\n",
      "Running pipeline for market: new-york-city\n",
      "Fetching data\n",
      "Applying clean_text function\n",
      "Generating sentiment scores\n",
      "Writing to Snowflake\n",
      "Data appended to table REVIEWS_SENTIMENT_SCORES in schema FEATURE_STORE at 2024-08-11 17:4116\n",
      "----------------------------------------------------------------\n",
      "Running pipeline for market: san-francisco\n",
      "Fetching data\n",
      "Applying clean_text function\n",
      "Generating sentiment scores\n",
      "Writing to Snowflake\n",
      "Data appended to table REVIEWS_SENTIMENT_SCORES in schema FEATURE_STORE at 2024-08-11 17:4610\n",
      "----------------------------------------------------------------\n",
      "Running pipeline for market: seattle\n",
      "Fetching data\n",
      "Applying clean_text function\n",
      "Generating sentiment scores\n",
      "Writing to Snowflake\n",
      "Data appended to table REVIEWS_SENTIMENT_SCORES in schema FEATURE_STORE at 2024-08-11 17:4952\n",
      "----------------------------------------------------------------\n",
      "Running pipeline for market: washington-dc\n",
      "Fetching data\n",
      "Applying clean_text function\n",
      "Generating sentiment scores\n",
      "Writing to Snowflake\n",
      "Data appended to table REVIEWS_SENTIMENT_SCORES in schema FEATURE_STORE at 2024-08-11 17:5248\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "markets = ['albany', 'chicago', 'los-angeles', 'new-york-city', 'san-francisco', 'seattle', 'washington-dc']\n",
    "\n",
    "df_test = process_and_upload_sentiment_scores(markets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close connection to Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
