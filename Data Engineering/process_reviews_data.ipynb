{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets import our requirements\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import logging\n",
    "import io\n",
    "import gzip\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# # load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Boto3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 23:10:01,334 - INFO - Found credentials in environment variables.\n"
     ]
    }
   ],
   "source": [
    "# boto3 will initialize connection using environment variables\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw/reviews/albany-reviews.csv.gz',\n",
       " 'raw/reviews/chicago-reviews.csv.gz',\n",
       " 'raw/reviews/los-angeles-reviews.csv.gz',\n",
       " 'raw/reviews/new-york-city-reviews.csv.gz',\n",
       " 'raw/reviews/san-francisco-reviews.csv.gz',\n",
       " 'raw/reviews/seattle-reviews.csv.gz',\n",
       " 'raw/reviews/washington-dc-reviews.csv.gz']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_files_in_folder(bucket_name, folder_prefix):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "    # List objects within the specified bucket and folder\n",
    "    obj_list = []\n",
    "    for obj in bucket.objects.filter(Prefix=folder_prefix):\n",
    "        # Check if the object is a file (not a folder)\n",
    "        if not obj.key.endswith('/'):\n",
    "            obj_list.append(obj.key)\n",
    "    return obj_list\n",
    "\n",
    "\n",
    "bucket_name = 'airbnb-capstone-project'\n",
    "folder_prefix = 'raw/reviews/'  # Specify the folder prefix\n",
    "items = list_files_in_folder(bucket_name, folder_prefix)\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Function to read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_review_data(file_path):\n",
    "    s3_base_url = f\"s3://{bucket_name}/\"\n",
    "\n",
    "    # dates to be parsed\n",
    "    date_columns = ['date']\n",
    "\n",
    "    df = pd.read_csv(s3_base_url + file_path, compression='gzip', parse_dates=date_columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Perform data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review_data(df_original, market_name):\n",
    "    # create copy to avoid modifying the original dataframe\n",
    "    df = df_original.copy()\n",
    "\n",
    "    # Add market name column\n",
    "    logging.info('Adding market name column')\n",
    "    df['market'] = market_name\n",
    "\n",
    "    # Change column name\n",
    "    logging.info('Converting column name')\n",
    "    df.rename(columns={'date':'review_date'}, inplace=True)\n",
    "\n",
    "    # Drop rows with empty review comments\n",
    "    logging.info('Removing rows with empty review comments')\n",
    "    df = df.dropna(subset=['comments'])\n",
    "    \n",
    "    # Drop reviewer_name column\n",
    "    logging.info('Dropping reviewer_name column')\n",
    "    df = df.drop(columns=['reviewer_name'])\n",
    "\n",
    "    # Check for duplicates\n",
    "    logging.info('Checking for duplicates')\n",
    "    duplicate_rows = df.duplicated().sum()\n",
    "    logging.info(f'Found {duplicate_rows} duplicate rows')\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Ensure review_date is a datetime type\n",
    "    logging.info('Ensuring review_date is a datetime type and converting to date only')\n",
    "    df['review_date'] = pd.to_datetime(df['review_date']).dt.date\n",
    "\n",
    "    logging.info('Data cleaning completed for market: %s', market_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Function to save to parqauet and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_cleaned_data_to_s3(df, bucket_name, market_name):\n",
    "    print('Upload starting...')\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # Convert DataFrame to Parquet in memory\n",
    "    parquet_buffer = io.BytesIO()\n",
    "    df.to_parquet(parquet_buffer, index=False, engine='pyarrow')\n",
    "\n",
    "    # Seek to the beginning of the buffer\n",
    "    parquet_buffer.seek(0)\n",
    "\n",
    "    # Construct the S3 key\n",
    "    s3_key = f\"processed/reviews/{market_name}-reviews_processed.parquet\"\n",
    "\n",
    "    # Upload the Parquet file to S3\n",
    "    s3.put_object(Bucket=bucket_name, Key=s3_key, Body=parquet_buffer.getvalue())\n",
    "    print(f\"File uploaded to S3 bucket '{bucket_name}' with key '{s3_key}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Main script to process files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(bucket_name, folder_prefix):\n",
    "    items = list_files_in_folder(bucket_name, folder_prefix)\n",
    "\n",
    "    for object in items:\n",
    "        # Extract market name from the file name\n",
    "        file_name = object.split('/')[-1]\n",
    "        market_name = '-'.join(file_name.split('-')[:-1])\n",
    "        \n",
    "        # Download the file from S3\n",
    "        logging.info(f'Downloading file: {object}')\n",
    "        df_calendar = read_review_data(object) # needs to be changed in utils script\n",
    "        \n",
    "        # Clean the data\n",
    "        df_calendar_cleaned = clean_review_data(df_calendar, market_name)\n",
    "        \n",
    "        # Upload the cleaned data to S3\n",
    "        upload_cleaned_data_to_s3(df_calendar_cleaned, bucket_name, market_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'airbnb-capstone-project'\n",
    "folder_prefix = 'raw/reviews/'\n",
    "\n",
    "list_files_in_folder(bucket_name, folder_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'airbnb-capstone-project'\n",
    "input_folder_path = 'raw/reviews/'\n",
    "\n",
    "\n",
    "process_files(bucket_name, input_folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
