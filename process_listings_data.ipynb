{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Listings Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets import our requirements\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import logging\n",
    "import io\n",
    "import gzip\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# # load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# Suppress FutureWarnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Boto3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:46:50,493 - INFO - Found credentials in environment variables.\n"
     ]
    }
   ],
   "source": [
    "# boto3 will initialize connection using environment variables\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Develop script to get files in each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw/listings/albany-listings.csv.gz',\n",
       " 'raw/listings/chicago-listings.csv.gz',\n",
       " 'raw/listings/los-angeles-listings.csv.gz',\n",
       " 'raw/listings/new-york-city-listings.csv.gz',\n",
       " 'raw/listings/san-francisco-listings.csv.gz',\n",
       " 'raw/listings/seattle-listings.csv.gz',\n",
       " 'raw/listings/washington-dc-listings.csv.gz']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_files_in_folder(bucket_name, folder_prefix):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "    # List objects within the specified bucket and folder\n",
    "    obj_list = []\n",
    "    for obj in bucket.objects.filter(Prefix=folder_prefix):\n",
    "        # Check if the object is a file (not a folder)\n",
    "        if not obj.key.endswith('/'):\n",
    "            obj_list.append(obj.key)\n",
    "    return obj_list\n",
    "\n",
    "\n",
    "bucket_name = 'airbnb-capstone-project'\n",
    "folder_prefix = 'raw/listings/'  # Specify the folder prefix\n",
    "items = list_files_in_folder(bucket_name, folder_prefix)\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Airbnb listings\n",
    "Load sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:46:51,478 - INFO - Found credentials in environment variables.\n"
     ]
    }
   ],
   "source": [
    "s3_path = 's3://airbnb-capstone-project/raw/listings/albany-listings.csv.gz'\n",
    "\n",
    "date_columns = ['last_scraped','host_since','calendar_last_scraped','first_review','last_review']\n",
    "\n",
    "df_listings = pd.read_csv(s3_path, compression='gzip', parse_dates=date_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'listing_url', 'scrape_id', 'last_scraped', 'source', 'name',\n",
       "       'description', 'neighborhood_overview', 'picture_url', 'host_id',\n",
       "       'host_url', 'host_name', 'host_since', 'host_location', 'host_about',\n",
       "       'host_response_time', 'host_response_rate', 'host_acceptance_rate',\n",
       "       'host_is_superhost', 'host_thumbnail_url', 'host_picture_url',\n",
       "       'host_neighbourhood', 'host_listings_count',\n",
       "       'host_total_listings_count', 'host_verifications',\n",
       "       'host_has_profile_pic', 'host_identity_verified', 'neighbourhood',\n",
       "       'neighbourhood_cleansed', 'neighbourhood_group_cleansed', 'latitude',\n",
       "       'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms',\n",
       "       'bathrooms_text', 'bedrooms', 'beds', 'amenities', 'price',\n",
       "       'minimum_nights', 'maximum_nights', 'minimum_minimum_nights',\n",
       "       'maximum_minimum_nights', 'minimum_maximum_nights',\n",
       "       'maximum_maximum_nights', 'minimum_nights_avg_ntm',\n",
       "       'maximum_nights_avg_ntm', 'calendar_updated', 'has_availability',\n",
       "       'availability_30', 'availability_60', 'availability_90',\n",
       "       'availability_365', 'calendar_last_scraped', 'number_of_reviews',\n",
       "       'number_of_reviews_ltm', 'number_of_reviews_l30d', 'first_review',\n",
       "       'last_review', 'review_scores_rating', 'review_scores_accuracy',\n",
       "       'review_scores_cleanliness', 'review_scores_checkin',\n",
       "       'review_scores_communication', 'review_scores_location',\n",
       "       'review_scores_value', 'license', 'instant_bookable',\n",
       "       'calculated_host_listings_count',\n",
       "       'calculated_host_listings_count_entire_homes',\n",
       "       'calculated_host_listings_count_private_rooms',\n",
       "       'calculated_host_listings_count_shared_rooms', 'reviews_per_month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_listings.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Create function to read in CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_listings_data(file_path):\n",
    "    s3_base_url = f\"s3://{bucket_name}/\"\n",
    "\n",
    "    # dates to be parsed\n",
    "    date_columns = ['last_scraped','host_since','calendar_last_scraped','first_review','last_review']\n",
    "\n",
    "    df = pd.read_csv(s3_base_url + file_path, compression='gzip', parse_dates=date_columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2.2 Create data cleaning function and add logging for data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_listings_data(df_original, market_name):\n",
    "    # create copy to avoid modifying the original dataframe\n",
    "    df = df_original.copy()\n",
    "\n",
    "    # Add market name column\n",
    "    logging.info('Adding market name column')\n",
    "    df['market'] = market_name\n",
    "\n",
    "    # Remove $ from price and convert to numeric\n",
    "    logging.info('Cleaning price and rate columns')\n",
    "    df['price'] = df['price'].str.replace('$', '')\n",
    "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "    df['host_response_rate'] = df['host_response_rate'].str.replace('%', '').astype(float)\n",
    "    df['host_acceptance_rate'] = df['host_acceptance_rate'].str.replace('%', '').astype(float)\n",
    "    \n",
    "\n",
    "    # Convert list of amenities into a string\n",
    "    logging.info('Converting amenities from list to string')\n",
    "    df['amenities'] = df['amenities'].apply(lambda x: x.replace('\"', '').replace('[', '').replace(']', '').replace('\\\\u2013', '-'))\n",
    "\n",
    "    # Convert superhost column from 'f' or 't' to 0 or 1, then to integer\n",
    "    logging.info('Converting array columns to string')\n",
    "    df['host_is_superhost'] = df['host_is_superhost'].replace({'f': 0, 't': 1})\n",
    "    df['host_is_superhost'] = df['host_is_superhost'].fillna(0).astype(int, errors='ignore')\n",
    "    df['host_verifications'] = df['host_verifications'].apply(lambda x: x.replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\") if pd.notnull(x) else x)\n",
    "\n",
    "    logging.info('Converting columns to string')\n",
    "    df['calendar_updated'] = df['calendar_updated'].astype('str')\n",
    "    df['license'] = df['license'].astype('str')\n",
    "\n",
    "\n",
    "    # Check for duplicates\n",
    "    logging.info('Checking for duplicates')\n",
    "    duplicate_rows = df.duplicated().sum()\n",
    "    logging.info(f'Found {duplicate_rows} duplicate rows')\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Ensure date columns are formatted as date type\n",
    "    logging.info('Converting date columns to date type')\n",
    "    date_columns = ['last_scraped','host_since','calendar_last_scraped','first_review','last_review']\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce').dt.date\n",
    "\n",
    "    # Drop unecessary columns\n",
    "    logging.info('Dropping unecessary columns')\n",
    "    columns_to_drop = ['source','calendar_updated','listing_url', 'picture_url','host_url','host_thumbnail_url', 'host_picture_url','neighbourhood_group_cleansed']\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "    logging.info('Data cleaning completed for market: %s', market_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Function to convert to parquet and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_cleaned_data_to_s3(df, bucket_name, market_name):\n",
    "    print('Upload starting...')\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    # Convert DataFrame to Parquet in memory\n",
    "    parquet_buffer = io.BytesIO()\n",
    "    df.to_parquet(parquet_buffer, index=False)\n",
    "\n",
    "    # Seek to the beginning of the buffer\n",
    "    parquet_buffer.seek(0)\n",
    "\n",
    "    # Construct the S3 key\n",
    "    s3_key = f\"processed/listings/{market_name}-listings_processed.parquet\"\n",
    "\n",
    "    # Upload the Parquet file to S3\n",
    "    s3.put_object(Bucket=bucket_name, Key=s3_key, Body=parquet_buffer.getvalue())\n",
    "    print(f\"File uploaded to S3 bucket '{bucket_name}' with key '{s3_key}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2.4 Main script to process files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(bucket_name, folder_prefix):\n",
    "    items = list_files_in_folder(bucket_name, folder_prefix)\n",
    "\n",
    "    for item in items:\n",
    "        # Extract market name from the file name\n",
    "        file_name = item.split('/')[-1]\n",
    "        market_name = '-'.join(file_name.split('-')[:-1])\n",
    "        \n",
    "        # Download the file from S3\n",
    "        logging.info(f'Downloading file: {item}')\n",
    "        df_listings = read_listings_data(item)\n",
    "        \n",
    "        # Clean the data\n",
    "        df_listings_cleaned = clean_listings_data(df_listings, market_name)\n",
    "        \n",
    "        # Upload the cleaned data to S3\n",
    "        upload_cleaned_data_to_s3(df_listings_cleaned, bucket_name, market_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process and upload files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw/listings/albany-listings.csv.gz',\n",
       " 'raw/listings/chicago-listings.csv.gz',\n",
       " 'raw/listings/los-angeles-listings.csv.gz',\n",
       " 'raw/listings/new-york-city-listings.csv.gz',\n",
       " 'raw/listings/san-francisco-listings.csv.gz',\n",
       " 'raw/listings/seattle-listings.csv.gz',\n",
       " 'raw/listings/washington-dc-listings.csv.gz']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_name = 'airbnb-capstone-project'\n",
    "folder_prefix = 'raw/listings/'\n",
    "\n",
    "list_files_in_folder(bucket_name, folder_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:46:55,915 - INFO - Downloading file: raw/listings/albany-listings.csv.gz\n",
      "2024-07-28 22:46:56,147 - INFO - Adding market name column\n",
      "2024-07-28 22:46:56,147 - INFO - Cleaning price and rate columns\n",
      "2024-07-28 22:46:56,147 - INFO - Converting amenities from list to string\n",
      "2024-07-28 22:46:56,147 - INFO - Converting array columns to string\n",
      "2024-07-28 22:46:56,147 - INFO - Converting columns to string\n",
      "2024-07-28 22:46:56,163 - INFO - Checking for duplicates\n",
      "2024-07-28 22:46:56,172 - INFO - Found 0 duplicate rows\n",
      "2024-07-28 22:46:56,172 - INFO - Converting date columns to date type\n",
      "2024-07-28 22:46:56,181 - INFO - Dropping unecessary columns\n",
      "2024-07-28 22:46:56,184 - INFO - Data cleaning completed for market: albany\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:46:58,671 - INFO - Downloading file: raw/listings/chicago-listings.csv.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded to S3 bucket 'airbnb-capstone-project' with key 'processed/listings/albany-listings_processed.parquet'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:47:00,532 - INFO - Adding market name column\n",
      "2024-07-28 22:47:00,534 - INFO - Cleaning price and rate columns\n",
      "2024-07-28 22:47:00,534 - INFO - Converting amenities from list to string\n",
      "2024-07-28 22:47:00,554 - INFO - Converting array columns to string\n",
      "2024-07-28 22:47:00,571 - INFO - Converting columns to string\n",
      "2024-07-28 22:47:00,573 - INFO - Checking for duplicates\n",
      "2024-07-28 22:47:00,608 - INFO - Found 0 duplicate rows\n",
      "2024-07-28 22:47:00,655 - INFO - Converting date columns to date type\n",
      "2024-07-28 22:47:00,718 - INFO - Dropping unecessary columns\n",
      "2024-07-28 22:47:00,718 - INFO - Data cleaning completed for market: chicago\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:47:08,390 - INFO - Downloading file: raw/listings/los-angeles-listings.csv.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded to S3 bucket 'airbnb-capstone-project' with key 'processed/listings/chicago-listings_processed.parquet'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:47:24,169 - INFO - Adding market name column\n",
      "2024-07-28 22:47:24,171 - INFO - Cleaning price and rate columns\n",
      "2024-07-28 22:47:24,187 - INFO - Converting amenities from list to string\n",
      "2024-07-28 22:47:24,308 - INFO - Converting array columns to string\n",
      "2024-07-28 22:47:24,325 - INFO - Converting columns to string\n",
      "2024-07-28 22:47:24,356 - INFO - Checking for duplicates\n",
      "2024-07-28 22:47:24,525 - INFO - Found 0 duplicate rows\n",
      "2024-07-28 22:47:24,728 - INFO - Converting date columns to date type\n",
      "2024-07-28 22:47:24,776 - INFO - Dropping unecessary columns\n",
      "2024-07-28 22:47:24,794 - INFO - Data cleaning completed for market: los-angeles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:48:03,103 - INFO - Downloading file: raw/listings/new-york-city-listings.csv.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded to S3 bucket 'airbnb-capstone-project' with key 'processed/listings/los-angeles-listings_processed.parquet'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:48:06,975 - INFO - Adding market name column\n",
      "2024-07-28 22:48:06,976 - INFO - Cleaning price and rate columns\n",
      "2024-07-28 22:48:07,001 - INFO - Converting amenities from list to string\n",
      "2024-07-28 22:48:07,061 - INFO - Converting array columns to string\n",
      "2024-07-28 22:48:07,073 - INFO - Converting columns to string\n",
      "2024-07-28 22:48:07,091 - INFO - Checking for duplicates\n",
      "2024-07-28 22:48:07,206 - INFO - Found 0 duplicate rows\n",
      "2024-07-28 22:48:07,373 - INFO - Converting date columns to date type\n",
      "2024-07-28 22:48:07,406 - INFO - Dropping unecessary columns\n",
      "2024-07-28 22:48:07,420 - INFO - Data cleaning completed for market: new-york-city\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:48:37,783 - INFO - Downloading file: raw/listings/san-francisco-listings.csv.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded to S3 bucket 'airbnb-capstone-project' with key 'processed/listings/new-york-city-listings_processed.parquet'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:48:39,366 - INFO - Adding market name column\n",
      "2024-07-28 22:48:39,367 - INFO - Cleaning price and rate columns\n",
      "2024-07-28 22:48:39,367 - INFO - Converting amenities from list to string\n",
      "2024-07-28 22:48:39,391 - INFO - Converting array columns to string\n",
      "2024-07-28 22:48:39,397 - INFO - Converting columns to string\n",
      "2024-07-28 22:48:39,400 - INFO - Checking for duplicates\n",
      "2024-07-28 22:48:39,432 - INFO - Found 0 duplicate rows\n",
      "2024-07-28 22:48:39,467 - INFO - Converting date columns to date type\n",
      "2024-07-28 22:48:39,487 - INFO - Dropping unecessary columns\n",
      "2024-07-28 22:48:39,487 - INFO - Data cleaning completed for market: san-francisco\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:48:46,891 - INFO - Downloading file: raw/listings/seattle-listings.csv.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded to S3 bucket 'airbnb-capstone-project' with key 'processed/listings/san-francisco-listings_processed.parquet'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:48:48,475 - INFO - Adding market name column\n",
      "2024-07-28 22:48:48,476 - INFO - Cleaning price and rate columns\n",
      "2024-07-28 22:48:48,481 - INFO - Converting amenities from list to string\n",
      "2024-07-28 22:48:48,496 - INFO - Converting array columns to string\n",
      "2024-07-28 22:48:48,508 - INFO - Converting columns to string\n",
      "2024-07-28 22:48:48,511 - INFO - Checking for duplicates\n",
      "2024-07-28 22:48:48,542 - INFO - Found 0 duplicate rows\n",
      "2024-07-28 22:48:48,576 - INFO - Converting date columns to date type\n",
      "2024-07-28 22:48:48,594 - INFO - Dropping unecessary columns\n",
      "2024-07-28 22:48:48,594 - INFO - Data cleaning completed for market: seattle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:48:55,265 - INFO - Downloading file: raw/listings/washington-dc-listings.csv.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded to S3 bucket 'airbnb-capstone-project' with key 'processed/listings/seattle-listings_processed.parquet'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 22:48:58,445 - INFO - Adding market name column\n",
      "2024-07-28 22:48:58,446 - INFO - Cleaning price and rate columns\n",
      "2024-07-28 22:48:58,448 - INFO - Converting amenities from list to string\n",
      "2024-07-28 22:48:58,448 - INFO - Converting array columns to string\n",
      "2024-07-28 22:48:58,469 - INFO - Converting columns to string\n",
      "2024-07-28 22:48:58,469 - INFO - Checking for duplicates\n",
      "2024-07-28 22:48:58,493 - INFO - Found 0 duplicate rows\n",
      "2024-07-28 22:48:58,520 - INFO - Converting date columns to date type\n",
      "2024-07-28 22:48:58,535 - INFO - Dropping unecessary columns\n",
      "2024-07-28 22:48:58,538 - INFO - Data cleaning completed for market: washington-dc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload starting...\n",
      "File uploaded to S3 bucket 'airbnb-capstone-project' with key 'processed/listings/washington-dc-listings_processed.parquet'\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'airbnb-capstone-project'\n",
    "input_folder_path = 'raw/listings/'\n",
    "process_files(bucket_name, input_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
